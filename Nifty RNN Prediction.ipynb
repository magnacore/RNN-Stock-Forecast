{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yahoo_fin\n",
    "#!pip install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = 4700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_stock_data(tickers, past_days = 365):\n",
    "    end = datetime.today()\n",
    "    start = end - timedelta(days=past_days)\n",
    "    \n",
    "    if len([tickers]) == 1:\n",
    "        # Import get_history function from nsepy module\n",
    "        from nsepy import get_history\n",
    "\n",
    "        data = get_history(symbol=tickers[0],\n",
    "                        start=start,\n",
    "                        end=end,\n",
    "                        futures=False,\n",
    "                        index=True)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
    "                test_size=0.2, feature_columns=['Close', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the data, default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = import_stock_data([ticker], past)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['Close'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                               test_size=test_size, shuffle=shuffle)\n",
    "    # return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size or the sequence length\n",
    "N_STEPS = 70\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 30\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\", \"Turnover\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1000\n",
    "# Tesla stock market\n",
    "ticker = \"NIFTY\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0167 - mean_absolute_error: 0.1269\n",
      "Epoch 00001: val_loss improved from inf to 0.00532, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0167 - mean_absolute_error: 0.1269 - val_loss: 0.0053 - val_mean_absolute_error: 0.0813\n",
      "Epoch 2/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0051 - mean_absolute_error: 0.0747\n",
      "Epoch 00002: val_loss improved from 0.00532 to 0.00319, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 0.0051 - mean_absolute_error: 0.0747 - val_loss: 0.0032 - val_mean_absolute_error: 0.0585\n",
      "Epoch 3/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0652\n",
      "Epoch 00003: val_loss did not improve from 0.00319\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 0.0039 - mean_absolute_error: 0.0653 - val_loss: 0.0033 - val_mean_absolute_error: 0.0565\n",
      "Epoch 4/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0040 - mean_absolute_error: 0.0638\n",
      "Epoch 00004: val_loss improved from 0.00319 to 0.00305, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.0040 - mean_absolute_error: 0.0638 - val_loss: 0.0031 - val_mean_absolute_error: 0.0551\n",
      "Epoch 5/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0633\n",
      "Epoch 00005: val_loss did not improve from 0.00305\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0038 - mean_absolute_error: 0.0635 - val_loss: 0.0036 - val_mean_absolute_error: 0.0664\n",
      "Epoch 6/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0041 - mean_absolute_error: 0.0661\n",
      "Epoch 00006: val_loss did not improve from 0.00305\n",
      "30/30 [==============================] - 3s 87ms/step - loss: 0.0041 - mean_absolute_error: 0.0662 - val_loss: 0.0031 - val_mean_absolute_error: 0.0535\n",
      "Epoch 7/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0633\n",
      "Epoch 00007: val_loss improved from 0.00305 to 0.00297, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 3s 96ms/step - loss: 0.0038 - mean_absolute_error: 0.0633 - val_loss: 0.0030 - val_mean_absolute_error: 0.0571\n",
      "Epoch 8/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0638\n",
      "Epoch 00008: val_loss did not improve from 0.00297\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.0039 - mean_absolute_error: 0.0639 - val_loss: 0.0031 - val_mean_absolute_error: 0.0533\n",
      "Epoch 9/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0616\n",
      "Epoch 00009: val_loss improved from 0.00297 to 0.00294, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.0035 - mean_absolute_error: 0.0615 - val_loss: 0.0029 - val_mean_absolute_error: 0.0515\n",
      "Epoch 10/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0651\n",
      "Epoch 00010: val_loss improved from 0.00294 to 0.00291, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0039 - mean_absolute_error: 0.0653 - val_loss: 0.0029 - val_mean_absolute_error: 0.0566\n",
      "Epoch 11/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0594\n",
      "Epoch 00011: val_loss did not improve from 0.00291\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0034 - mean_absolute_error: 0.0594 - val_loss: 0.0039 - val_mean_absolute_error: 0.0612\n",
      "Epoch 12/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0587\n",
      "Epoch 00012: val_loss improved from 0.00291 to 0.00281, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.0034 - mean_absolute_error: 0.0587 - val_loss: 0.0028 - val_mean_absolute_error: 0.0556\n",
      "Epoch 13/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0585\n",
      "Epoch 00013: val_loss improved from 0.00281 to 0.00265, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 0.0033 - mean_absolute_error: 0.0585 - val_loss: 0.0027 - val_mean_absolute_error: 0.0511\n",
      "Epoch 14/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0586\n",
      "Epoch 00014: val_loss did not improve from 0.00265\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.0032 - mean_absolute_error: 0.0585 - val_loss: 0.0036 - val_mean_absolute_error: 0.0579\n",
      "Epoch 15/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0612\n",
      "Epoch 00015: val_loss improved from 0.00265 to 0.00254, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.0035 - mean_absolute_error: 0.0612 - val_loss: 0.0025 - val_mean_absolute_error: 0.0494\n",
      "Epoch 16/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0031 - mean_absolute_error: 0.0572\n",
      "Epoch 00016: val_loss did not improve from 0.00254\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.0031 - mean_absolute_error: 0.0571 - val_loss: 0.0029 - val_mean_absolute_error: 0.0581\n",
      "Epoch 17/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0572\n",
      "Epoch 00017: val_loss improved from 0.00254 to 0.00244, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0032 - mean_absolute_error: 0.0572 - val_loss: 0.0024 - val_mean_absolute_error: 0.0487\n",
      "Epoch 18/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0030 - mean_absolute_error: 0.0565\n",
      "Epoch 00018: val_loss did not improve from 0.00244\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.0031 - mean_absolute_error: 0.0566 - val_loss: 0.0025 - val_mean_absolute_error: 0.0529\n",
      "Epoch 19/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0030 - mean_absolute_error: 0.0552\n",
      "Epoch 00019: val_loss improved from 0.00244 to 0.00238, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0030 - mean_absolute_error: 0.0551 - val_loss: 0.0024 - val_mean_absolute_error: 0.0502\n",
      "Epoch 20/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0030 - mean_absolute_error: 0.0561\n",
      "Epoch 00020: val_loss did not improve from 0.00238\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 0.0030 - mean_absolute_error: 0.0561 - val_loss: 0.0028 - val_mean_absolute_error: 0.0577\n",
      "Epoch 21/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0029 - mean_absolute_error: 0.0548\n",
      "Epoch 00021: val_loss improved from 0.00238 to 0.00233, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0029 - mean_absolute_error: 0.0550 - val_loss: 0.0023 - val_mean_absolute_error: 0.0493\n",
      "Epoch 22/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0536\n",
      "Epoch 00022: val_loss improved from 0.00233 to 0.00217, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.0027 - mean_absolute_error: 0.0536 - val_loss: 0.0022 - val_mean_absolute_error: 0.0459\n",
      "Epoch 23/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0026 - mean_absolute_error: 0.0526\n",
      "Epoch 00023: val_loss did not improve from 0.00217\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.0026 - mean_absolute_error: 0.0525 - val_loss: 0.0022 - val_mean_absolute_error: 0.0487\n",
      "Epoch 24/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0529\n",
      "Epoch 00024: val_loss did not improve from 0.00217\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.0028 - mean_absolute_error: 0.0531 - val_loss: 0.0023 - val_mean_absolute_error: 0.0451\n",
      "Epoch 25/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0029 - mean_absolute_error: 0.0563\n",
      "Epoch 00025: val_loss did not improve from 0.00217\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 0.0029 - mean_absolute_error: 0.0564 - val_loss: 0.0025 - val_mean_absolute_error: 0.0486\n",
      "Epoch 26/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0541\n",
      "Epoch 00026: val_loss improved from 0.00217 to 0.00213, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 0.0027 - mean_absolute_error: 0.0542 - val_loss: 0.0021 - val_mean_absolute_error: 0.0467\n",
      "Epoch 27/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0530\n",
      "Epoch 00027: val_loss improved from 0.00213 to 0.00208, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.0027 - mean_absolute_error: 0.0529 - val_loss: 0.0021 - val_mean_absolute_error: 0.0457\n",
      "Epoch 28/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0025 - mean_absolute_error: 0.0525\n",
      "Epoch 00028: val_loss improved from 0.00208 to 0.00200, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.0025 - mean_absolute_error: 0.0525 - val_loss: 0.0020 - val_mean_absolute_error: 0.0444\n",
      "Epoch 29/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0028 - mean_absolute_error: 0.0548\n",
      "Epoch 00029: val_loss did not improve from 0.00200\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 0.0028 - mean_absolute_error: 0.0549 - val_loss: 0.0021 - val_mean_absolute_error: 0.0438\n",
      "Epoch 30/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0024 - mean_absolute_error: 0.0512\n",
      "Epoch 00030: val_loss did not improve from 0.00200\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 0.0024 - mean_absolute_error: 0.0513 - val_loss: 0.0023 - val_mean_absolute_error: 0.0521\n",
      "Epoch 31/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0027 - mean_absolute_error: 0.0538\n",
      "Epoch 00031: val_loss did not improve from 0.00200\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.0027 - mean_absolute_error: 0.0537 - val_loss: 0.0021 - val_mean_absolute_error: 0.0441\n",
      "Epoch 32/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0024 - mean_absolute_error: 0.0503\n",
      "Epoch 00032: val_loss improved from 0.00200 to 0.00200, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.0024 - mean_absolute_error: 0.0503 - val_loss: 0.0020 - val_mean_absolute_error: 0.0462\n",
      "Epoch 33/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0025 - mean_absolute_error: 0.0518\n",
      "Epoch 00033: val_loss improved from 0.00200 to 0.00193, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 3s 88ms/step - loss: 0.0025 - mean_absolute_error: 0.0519 - val_loss: 0.0019 - val_mean_absolute_error: 0.0457\n",
      "Epoch 34/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0026 - mean_absolute_error: 0.0522\n",
      "Epoch 00034: val_loss did not improve from 0.00193\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.0025 - mean_absolute_error: 0.0521 - val_loss: 0.0020 - val_mean_absolute_error: 0.0426\n",
      "Epoch 35/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0024 - mean_absolute_error: 0.0512\n",
      "Epoch 00035: val_loss improved from 0.00193 to 0.00192, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.0024 - mean_absolute_error: 0.0512 - val_loss: 0.0019 - val_mean_absolute_error: 0.0444\n",
      "Epoch 36/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0025 - mean_absolute_error: 0.0508\n",
      "Epoch 00036: val_loss improved from 0.00192 to 0.00178, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.0025 - mean_absolute_error: 0.0507 - val_loss: 0.0018 - val_mean_absolute_error: 0.0433\n",
      "Epoch 37/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0489\n",
      "Epoch 00037: val_loss did not improve from 0.00178\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 0.0022 - mean_absolute_error: 0.0489 - val_loss: 0.0020 - val_mean_absolute_error: 0.0440\n",
      "Epoch 38/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0484\n",
      "Epoch 00038: val_loss did not improve from 0.00178\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.0022 - mean_absolute_error: 0.0484 - val_loss: 0.0019 - val_mean_absolute_error: 0.0460\n",
      "Epoch 39/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0484\n",
      "Epoch 00039: val_loss did not improve from 0.00178\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 0.0021 - mean_absolute_error: 0.0485 - val_loss: 0.0018 - val_mean_absolute_error: 0.0450\n",
      "Epoch 40/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0501\n",
      "Epoch 00040: val_loss did not improve from 0.00178\n",
      "30/30 [==============================] - 3s 94ms/step - loss: 0.0023 - mean_absolute_error: 0.0501 - val_loss: 0.0019 - val_mean_absolute_error: 0.0459\n",
      "Epoch 41/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0496\n",
      "Epoch 00041: val_loss improved from 0.00178 to 0.00170, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 3s 94ms/step - loss: 0.0022 - mean_absolute_error: 0.0496 - val_loss: 0.0017 - val_mean_absolute_error: 0.0437\n",
      "Epoch 42/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0500\n",
      "Epoch 00042: val_loss improved from 0.00170 to 0.00164, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 0.0023 - mean_absolute_error: 0.0498 - val_loss: 0.0016 - val_mean_absolute_error: 0.0425\n",
      "Epoch 43/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0492\n",
      "Epoch 00043: val_loss did not improve from 0.00164\n",
      "30/30 [==============================] - 3s 83ms/step - loss: 0.0021 - mean_absolute_error: 0.0492 - val_loss: 0.0018 - val_mean_absolute_error: 0.0421\n",
      "Epoch 44/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0487\n",
      "Epoch 00044: val_loss did not improve from 0.00164\n",
      "30/30 [==============================] - 3s 90ms/step - loss: 0.0021 - mean_absolute_error: 0.0486 - val_loss: 0.0016 - val_mean_absolute_error: 0.0417\n",
      "Epoch 45/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0484\n",
      "Epoch 00045: val_loss did not improve from 0.00164\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.0021 - mean_absolute_error: 0.0484 - val_loss: 0.0021 - val_mean_absolute_error: 0.0488\n",
      "Epoch 46/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0480\n",
      "Epoch 00046: val_loss improved from 0.00164 to 0.00161, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 0.0021 - mean_absolute_error: 0.0480 - val_loss: 0.0016 - val_mean_absolute_error: 0.0413\n",
      "Epoch 47/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0488\n",
      "Epoch 00047: val_loss did not improve from 0.00161\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 0.0021 - mean_absolute_error: 0.0487 - val_loss: 0.0020 - val_mean_absolute_error: 0.0436\n",
      "Epoch 48/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0488\n",
      "Epoch 00048: val_loss did not improve from 0.00161\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 0.0022 - mean_absolute_error: 0.0488 - val_loss: 0.0019 - val_mean_absolute_error: 0.0443\n",
      "Epoch 49/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0019 - mean_absolute_error: 0.0465\n",
      "Epoch 00049: val_loss did not improve from 0.00161\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 0.0019 - mean_absolute_error: 0.0465 - val_loss: 0.0017 - val_mean_absolute_error: 0.0401\n",
      "Epoch 50/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0471\n",
      "Epoch 00050: val_loss did not improve from 0.00161\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 0.0020 - mean_absolute_error: 0.0471 - val_loss: 0.0021 - val_mean_absolute_error: 0.0441\n",
      "Epoch 51/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0026 - mean_absolute_error: 0.0522\n",
      "Epoch 00051: val_loss did not improve from 0.00161\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.0026 - mean_absolute_error: 0.0521 - val_loss: 0.0024 - val_mean_absolute_error: 0.0520\n",
      "Epoch 52/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0506\n",
      "Epoch 00052: val_loss improved from 0.00161 to 0.00153, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.0022 - mean_absolute_error: 0.0506 - val_loss: 0.0015 - val_mean_absolute_error: 0.0412\n",
      "Epoch 53/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0471\n",
      "Epoch 00053: val_loss did not improve from 0.00153\n",
      "30/30 [==============================] - 3s 87ms/step - loss: 0.0020 - mean_absolute_error: 0.0471 - val_loss: 0.0018 - val_mean_absolute_error: 0.0436\n",
      "Epoch 54/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0473\n",
      "Epoch 00054: val_loss did not improve from 0.00153\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0020 - mean_absolute_error: 0.0473 - val_loss: 0.0017 - val_mean_absolute_error: 0.0445\n",
      "Epoch 55/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0467\n",
      "Epoch 00055: val_loss did not improve from 0.00153\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 0.0019 - mean_absolute_error: 0.0466 - val_loss: 0.0017 - val_mean_absolute_error: 0.0440\n",
      "Epoch 56/1000\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0485\n",
      "Epoch 00056: val_loss did not improve from 0.00153\n",
      "30/30 [==============================] - 3s 96ms/step - loss: 0.0023 - mean_absolute_error: 0.0485 - val_loss: 0.0018 - val_mean_absolute_error: 0.0445\n",
      "Epoch 57/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0019 - mean_absolute_error: 0.0462\n",
      "Epoch 00057: val_loss improved from 0.00153 to 0.00141, saving model to results/2020-12-01_NIFTY-huber_loss-adam-LSTM-seq-70-step-30-layers-3-units-256.h5\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 0.0019 - mean_absolute_error: 0.0462 - val_loss: 0.0014 - val_mean_absolute_error: 0.0392\n",
      "Epoch 58/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0471\n",
      "Epoch 00058: val_loss did not improve from 0.00141\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.0020 - mean_absolute_error: 0.0471 - val_loss: 0.0020 - val_mean_absolute_error: 0.0483\n",
      "Epoch 59/1000\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0475\n",
      "Epoch 00059: val_loss did not improve from 0.00141\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 0.0020 - mean_absolute_error: 0.0475 - val_loss: 0.0015 - val_mean_absolute_error: 0.0428\n",
      "CPU times: user 11min 45s, sys: 5min 22s, total: 17min 8s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard, early_stopping],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=\"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 5098.335201922432\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "mean_absolute_error = data[\"column_scaler\"][\"Close\"].inverse_transform([[mae]])[0][0]\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # retrieve the column scalers\n",
    "    column_scaler = data[\"column_scaler\"]\n",
    "    # reshape the last sequence\n",
    "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    predicted_price = column_scaler[\"Close\"].inverse_transform(prediction)[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 30 days is 11326.93 Rs\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f} Rs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"Close\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"Close\"].inverse_transform(y_pred))\n",
    "    # last 200 days, feel free to edit that\n",
    "    plt.plot(y_test[-200:], c='b')\n",
    "    plt.plot(y_pred[-200:], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"Close\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"Close\"].inverse_transform(y_pred))\n",
    "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
    "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: Accuracy Score: 0.5968109339407744\n"
     ]
    }
   ],
   "source": [
    "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
